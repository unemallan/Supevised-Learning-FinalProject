# SupweviseLearning_FinalProject
UC_Boulder Supervised Learning Final Project
This Codebase uses the dataset from https://www.kaggle.com/c/customer-churn-prediction-2020/data

## Project Title: Predicting Customer Churn Using Kaggle Data

## Background:
Customer churn, the loss of clients or customers, is a critical metric for businesses in subscription-based industries like telecom, or D2C business. Predicting likely churn for a specific customer can aid in implementing mitigations to retain them.

## Objective:
The Final project reuquires the creation of a supervised learning model. Here we will create predictions using Boosting model to predict customer churn based on historical data.

## Required Tasks Per Rubric:

#### Data Preprocessing: Clean the data, handle missing values, encode categorical variables, and scale numerical features if necessary.
#### Exploratory Data Analysis (EDA): Explore the dataset to gain insights into the relationship between features and churn. Visualize the data to understand patterns and correlations.
#### Feature Selection: Identify the most relevant features for predicting churn using techniques like feature importance or correlation analysis.
#### Model Selection: Experiment with different supervised learning algorithms such as Logistic Regression, Random Forest, Support Vector Machines (SVM), Gradient Boosting, etc.
#### Model Training: Split the data into training and testing sets. Train the selected models on the training data.
#### Model Evaluation: Evaluate the performance of each model using appropriate evaluation metrics such as accuracy, precision, recall, F1-score, and ROC-AUC curve.
#### Hyperparameter Tuning: Fine-tune the hyperparameters of the best-performing model to improve its performance further.
#### Model Interpretation: Interpret the trained model to understand which features are most influential in predicting churn.

## Deliverables:
#### A Jupyter notebook showing:
*  Supervised learning problem description,
*  EDA procedure
*  Analysis (model building and training)
*  result,
*  Discussion/Conclusion.

## Future Work:

* Implementing ensemble methods like stacking or blending to improve model performance
* Test Model peromance against other methods like SVM, and Binary Classification
